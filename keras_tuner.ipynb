{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mKVO09c-nCc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "# Function to build the model\n",
        "def build_model(hp):\n",
        "    # Load InceptionResNetV2 as the base model\n",
        "    inception_resnet = InceptionResNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "    # Build the head model\n",
        "    headmodel = inception_resnet.output\n",
        "    headmodel = Flatten()(headmodel)\n",
        "    headmodel = Dense(hp.Int('units_1', min_value=256, max_value=1024, step=32), activation=\"relu\")(headmodel)\n",
        "    headmodel = Dense(hp.Int('units_2', min_value=32, max_value=256, step=32), activation=\"relu\")(headmodel)\n",
        "    headmodel = Dense(4, activation='sigmoid')(headmodel)\n",
        "\n",
        "    # Build the complete model\n",
        "    model = Model(inputs=inception_resnet.input, outputs=headmodel)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-6, max_value=1e-3, sampling='log')),\n",
        "        loss='mean_squared_error',  # Adjust the loss function based on your specific task\n",
        "\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=7,  # Adjust the number of trials based on your computational resources\n",
        "    directory='my_tuning_dir',\n",
        "    project_name='object_detection_tuning'\n",
        ")\n",
        "\n",
        "# Define EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Perform the hyperparameter search\n",
        "tuner.search(x=x_train,y=y_train,batch_size=10,epochs=180,\n",
        "                    validation_data=(x_test,y_test), callbacks=[early_stopping])  # Adjust epochs and validation split as needed\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyperparameters: {best_hps}\")\n",
        "\n",
        "# Build the final model with the best hyperparameters\n",
        "final_model = tuner.hypermodel.build(best_hps)\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(f\"Units 1: {best_hps.get('units_1')}\")\n",
        "print(f\"Units 2: {best_hps.get('units_2')}\")\n",
        "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")"
      ]
    }
  ]
}